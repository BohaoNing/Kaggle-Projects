{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### This solution is inspired by [Alexandru Papiu](https://www.kaggle.com/code/apapiu/regularized-linear-models), [Serigne](https://www.kaggle.com/code/serigne/stacked-regressions-top-4-on-leaderboard/notebook) and [Lavanya Shukla](https://www.kaggle.com/code/lavanyashukla01/how-i-made-top-0-3-on-a-kaggle-competition/notebook)","metadata":{}},{"cell_type":"code","source":"# Essentials\nimport numpy as np\nimport pandas as pd\nimport datetime\nimport random\n\n# Plots\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Models\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.linear_model import Ridge, RidgeCV\nfrom sklearn.linear_model import ElasticNet, ElasticNetCV\nfrom sklearn.svm import SVR\nfrom mlxtend.regressor import StackingCVRegressor\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\n\n# Stats\nfrom scipy.stats import skew, norm\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\n# Misc\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import scale\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.decomposition import PCA\n\npd.set_option('display.max_columns', None)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-21T18:30:21.501295Z","iopub.execute_input":"2022-07-21T18:30:21.501668Z","iopub.status.idle":"2022-07-21T18:30:21.517037Z","shell.execute_reply.started":"2022-07-21T18:30:21.501636Z","shell.execute_reply":"2022-07-21T18:30:21.515865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Imports & EDA","metadata":{}},{"cell_type":"code","source":"sample_submission = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/sample_submission.csv')\ntrain_data = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\ntest_data = pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\n\ntrain = train_data\ntest = test_data\n\ntrain_test_data = pd.concat([train, test],axis=0,ignore_index=True)\ntrain_test_data.describe()","metadata":{"execution":{"iopub.status.busy":"2022-07-21T18:24:11.731937Z","iopub.execute_input":"2022-07-21T18:24:11.732718Z","iopub.status.idle":"2022-07-21T18:24:11.922246Z","shell.execute_reply.started":"2022-07-21T18:24:11.732677Z","shell.execute_reply":"2022-07-21T18:24:11.921124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Missing ratio of each feature\nmissing_ratio = train_test_data.isnull().sum() / len(train_test_data)\nmissing_ratio = missing_ratio[missing_ratio.values>0]\nmissing_ratio_low = missing_ratio[missing_ratio.values<=0.1]\nmissing_ratio_high = missing_ratio[missing_ratio.values>0.1]\n\nfig = plt.figure(figsize=(10,4),dpi=100)\nplt.subplot(1,2,1)\nsns.barplot(x=missing_ratio_low.index,y=missing_ratio_low.values)\nplt.xticks(rotation=90)\nplt.subplot(1,2,2)\nsns.barplot(x=missing_ratio_high.index,y=missing_ratio_high.values)\nplt.xticks(rotation=90);","metadata":{"execution":{"iopub.status.busy":"2022-07-21T18:24:14.725122Z","iopub.execute_input":"2022-07-21T18:24:14.725663Z","iopub.status.idle":"2022-07-21T18:24:15.381820Z","shell.execute_reply.started":"2022-07-21T18:24:14.725617Z","shell.execute_reply":"2022-07-21T18:24:15.380341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(6,4),dpi=100)\nsns.heatmap(train_test_data.corr(),cmap='viridis')","metadata":{"execution":{"iopub.status.busy":"2022-07-21T18:24:15.477457Z","iopub.execute_input":"2022-07-21T18:24:15.478391Z","iopub.status.idle":"2022-07-21T18:24:16.048298Z","shell.execute_reply.started":"2022-07-21T18:24:15.478334Z","shell.execute_reply":"2022-07-21T18:24:16.046835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (12.0, 6.0)\nprices = pd.DataFrame({\"price\":train[\"SalePrice\"], \"log(price + 1)\":np.log1p(train[\"SalePrice\"])})\nprices.hist()","metadata":{"execution":{"iopub.status.busy":"2022-07-21T18:24:27.634498Z","iopub.execute_input":"2022-07-21T18:24:27.635428Z","iopub.status.idle":"2022-07-21T18:24:28.101354Z","shell.execute_reply.started":"2022-07-21T18:24:27.635383Z","shell.execute_reply":"2022-07-21T18:24:28.099898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Feature Engineering","metadata":{}},{"cell_type":"code","source":"# Fill in missing rows\ntrain['LotFrontage'] = train.groupby('Neighborhood')['LotFrontage'].transform(lambda val: val.fillna(val.mean()))\ntrain['MiscFeature'] = train['MiscFeature'].replace(to_replace=['Gar2','Othr','TenC'],value='Other')\ntrain['MiscFeature'] = train['MiscFeature'].fillna(value='Other')\n\ntest['LotFrontage'] = test.groupby('Neighborhood')['LotFrontage'].transform(lambda val: val.fillna(val.mean()))\ntest['MiscFeature'] = test['MiscFeature'].replace(to_replace=['Gar2','Othr','TenC'],value='Other')\ntest['MiscFeature'] = test['MiscFeature'].fillna(value='Other')\n\n# Drop some features with high missing ratio\ntrain = train.drop(['Alley','FireplaceQu','PoolQC','Fence'],axis=1)\ntest = test.drop(['Alley','FireplaceQu','PoolQC','Fence'],axis=1)\n\n# Fill in missing rows\ntrain['MSZoning'] = train['MSZoning'].fillna(value='RL')\ntrain['Exterior1st'] = train['Exterior1st'].fillna(value='VinylSd')\ntrain['Exterior2nd'] = train['Exterior2nd'].fillna(value='VinylSd')\ntrain['MasVnrType'] = train['MasVnrType'].fillna(value='None')\ntrain['MasVnrArea'] = train['MasVnrArea'].fillna(value=0)\ntrain[['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2']] = train[['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2']].fillna(value='None')\ntrain[['BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','BsmtFullBath','BsmtHalfBath']] = train[['BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','BsmtFullBath','BsmtHalfBath']].fillna(value=0)\ntrain['Electrical'] = train['Electrical'].fillna(value='SBrkr')\ntrain['KitchenQual'] = train['KitchenQual'].fillna(value='TA')\ntrain['Functional'] = train['Functional'].fillna(value='Typ')\ntrain[['GarageType','GarageFinish','GarageQual','GarageCond']] = train[['GarageType','GarageFinish','GarageQual','GarageCond']].fillna(value='None')\ntrain[['GarageYrBlt','GarageCars','GarageArea']] = train[['GarageYrBlt','GarageCars','GarageArea']].fillna(value=0)\ntrain['SaleType'] = train['SaleType'].fillna(value='WD')\n\ntest['MSZoning'] = test['MSZoning'].fillna(value='RL')\ntest['Exterior1st'] = test['Exterior1st'].fillna(value='VinylSd')\ntest['Exterior2nd'] = test['Exterior2nd'].fillna(value='VinylSd')\ntest['MasVnrType'] = test['MasVnrType'].fillna(value='None')\ntest['MasVnrArea'] = test['MasVnrArea'].fillna(value=0)\ntest[['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2']] = test[['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2']].fillna(value='None')\ntest[['BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','BsmtFullBath','BsmtHalfBath']] = test[['BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','BsmtFullBath','BsmtHalfBath']].fillna(value=0)\ntest['Electrical'] = test['Electrical'].fillna(value='SBrkr')\ntest['KitchenQual'] = test['KitchenQual'].fillna(value='TA')\ntest['Functional'] = test['Functional'].fillna(value='Typ')\ntest[['GarageType','GarageFinish','GarageQual','GarageCond']] = test[['GarageType','GarageFinish','GarageQual','GarageCond']].fillna(value='None')\ntest[['GarageYrBlt','GarageCars','GarageArea']] = test[['GarageYrBlt','GarageCars','GarageArea']].fillna(value=0)\ntest['SaleType'] = test['SaleType'].fillna(value='WD')\n\n# Drop some features\ntrain = train.drop(['Utilities','Id'],axis=1)\ntest = test.drop(['Utilities','Id'],axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-07-21T18:24:30.357755Z","iopub.execute_input":"2022-07-21T18:24:30.358139Z","iopub.status.idle":"2022-07-21T18:24:30.439354Z","shell.execute_reply.started":"2022-07-21T18:24:30.358106Z","shell.execute_reply":"2022-07-21T18:24:30.438382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data Scaling\n\n# Log transform the target\ntrain['SalePrice'] = np.log1p(train['SalePrice'])\n\n# Log/BoxCox transform skewed numeric features\nnumeric_feats = test.dtypes[test.dtypes != \"object\"].index\n\nskewed_feats = test[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\nskewed_feats = skewed_feats[skewed_feats > 0.75]\nskewed_feats = skewed_feats.index\n\n# train_test_data[skewed_feats] = np.log1p(train_test_data[skewed_feats])\nlam = 0.15\nfor feat in skewed_feats:\n    train[feat] = boxcox1p(train[feat], lam)\n    test[feat] = boxcox1p(test[feat], lam)","metadata":{"execution":{"iopub.status.busy":"2022-07-21T18:24:32.978657Z","iopub.execute_input":"2022-07-21T18:24:32.979033Z","iopub.status.idle":"2022-07-21T18:24:33.023993Z","shell.execute_reply.started":"2022-07-21T18:24:32.979001Z","shell.execute_reply":"2022-07-21T18:24:33.022980Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = train['SalePrice']\ntrain = train.drop('SalePrice',axis=1)\n\ntrain_test_data = pd.concat([train, test],axis=0,ignore_index=True)\ntrain_test_data = pd.get_dummies(train_test_data,drop_first=True)\n\nX = train_test_data.iloc[:1460]\nX_submission = train_test_data.iloc[1460:]","metadata":{"execution":{"iopub.status.busy":"2022-07-21T18:24:34.287707Z","iopub.execute_input":"2022-07-21T18:24:34.288223Z","iopub.status.idle":"2022-07-21T18:24:34.361444Z","shell.execute_reply.started":"2022-07-21T18:24:34.288176Z","shell.execute_reply":"2022-07-21T18:24:34.360246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Modeling","metadata":{}},{"cell_type":"code","source":"# Setup cross validation folds\nkf = KFold(n_splits=12, random_state=42, shuffle=True)\n\n# Define error metrics\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef cv_rmse(model, X=X):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kf))\n    return (rmse)","metadata":{"execution":{"iopub.status.busy":"2022-07-21T18:24:37.121973Z","iopub.execute_input":"2022-07-21T18:24:37.122401Z","iopub.status.idle":"2022-07-21T18:24:37.129868Z","shell.execute_reply.started":"2022-07-21T18:24:37.122365Z","shell.execute_reply":"2022-07-21T18:24:37.128555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Light Gradient Boosting Regressor\nlightgbm = LGBMRegressor(objective='regression', \n                       num_leaves=6,\n                       learning_rate=0.01, \n                       n_estimators=7000,\n                       max_bin=200, \n                       bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.2,\n                       feature_fraction_seed=8,\n                       min_sum_hessian_in_leaf = 11,\n                       verbose=-1,\n                       random_state=42)\n\n# XGBoost Regressor\nxgboost = XGBRegressor(learning_rate=0.01,\n                       n_estimators=6000,\n                       max_depth=4,\n                       min_child_weight=0,\n                       gamma=0.6,\n                       subsample=0.7,\n                       colsample_bytree=0.7,\n                       objective='reg:squarederror',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       seed=27,\n                       reg_alpha=0.00006,\n                       random_state=42)\n\n# Ridge Regressor\nridge_alphas = [1e-15, 1e-10, 1e-8, 9e-4, 7e-4, 5e-4, 3e-4, 1e-4, 1e-3, 5e-2, 1e-2, 0.1, 0.3, 1, 3, 5, 10, 15, 18, 20, 30, 50, 75, 100]\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=ridge_alphas, cv=kf))\n\n# Support Vector Regressor\nsvr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003))\n\n# Gradient Boosting Regressor\ngbr = GradientBoostingRegressor(n_estimators=6000,\n                                learning_rate=0.01,\n                                max_depth=4,\n                                max_features='sqrt',\n                                min_samples_leaf=15,\n                                min_samples_split=10,\n                                loss='huber',\n                                random_state=42)  \n\n# Random Forest Regressor\nrf = RandomForestRegressor(n_estimators=1200,\n                          max_depth=15,\n                          min_samples_split=5,\n                          min_samples_leaf=5,\n                          max_features=None,\n                          oob_score=True,\n                          random_state=42)\n\n# Stack up all the models above, optimized using xgboost\nstack_gen = StackingCVRegressor(regressors=(xgboost, lightgbm, svr, ridge, gbr, rf),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)","metadata":{"execution":{"iopub.status.busy":"2022-07-21T18:24:40.471631Z","iopub.execute_input":"2022-07-21T18:24:40.473009Z","iopub.status.idle":"2022-07-21T18:24:40.488917Z","shell.execute_reply.started":"2022-07-21T18:24:40.472950Z","shell.execute_reply":"2022-07-21T18:24:40.487925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get cross validation scores for each model\nscores = {}\n\n# score = cv_rmse(lightgbm)\n# print(\"lightgbm: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n# scores['lgb'] = (score.mean(), score.std())\n\n# score = cv_rmse(xgboost)\n# print(\"xgboost: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n# scores['xgb'] = (score.mean(), score.std())\n\n# score = cv_rmse(svr)\n# print(\"SVR: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n# scores['svr'] = (score.mean(), score.std())\n\n# score = cv_rmse(ridge)\n# print(\"ridge: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n# scores['ridge'] = (score.mean(), score.std())\n\n# score = cv_rmse(rf)\n# print(\"rf: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n# scores['rf'] = (score.mean(), score.std())\n\n# score = cv_rmse(gbr)\n# print(\"gbr: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n# scores['gbr'] = (score.mean(), score.std())","metadata":{"execution":{"iopub.status.busy":"2022-07-21T17:24:05.476566Z","iopub.execute_input":"2022-07-21T17:24:05.476951Z","iopub.status.idle":"2022-07-21T17:46:10.940381Z","shell.execute_reply.started":"2022-07-21T17:24:05.476920Z","shell.execute_reply":"2022-07-21T17:46:10.939094Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit the models\nprint('stack_gen')\nstack_gen_model = stack_gen.fit(np.array(X), np.array(y))\n\nprint('lightgbm')\nlgb_model_full_data = lightgbm.fit(X, y)\n\nprint('xgboost')\nxgb_model_full_data = xgboost.fit(X, y)\n\nprint('Svr')\nsvr_model_full_data = svr.fit(X, y)\n\nprint('Ridge')\nridge_model_full_data = ridge.fit(X, y)\n\nprint('RandomForest')\nrf_model_full_data = rf.fit(X, y)\n\nprint('GradientBoosting')\ngbr_model_full_data = gbr.fit(X, y)","metadata":{"execution":{"iopub.status.busy":"2022-07-21T18:24:43.294437Z","iopub.execute_input":"2022-07-21T18:24:43.294869Z","iopub.status.idle":"2022-07-21T18:26:39.648291Z","shell.execute_reply.started":"2022-07-21T18:24:43.294829Z","shell.execute_reply":"2022-07-21T18:26:39.647173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the scores for each model\n# sns.set_style(\"white\")\n# fig = plt.figure(figsize=(10, 5))\n\n# ax = sns.pointplot(x=list(scores.keys()), y=[score for score, _ in scores.values()], markers=['o'], linestyles=['-'])\n# for i, score in enumerate(scores.values()):\n#     ax.text(i, score[0] + 0.002, '{:.6f}'.format(score[0]), horizontalalignment='left', size='large', color='black', weight='semibold')\n\n# plt.ylabel('Score (RMSE)', size=20, labelpad=12.5)\n# plt.xlabel('Model', size=20, labelpad=12.5)\n# plt.tick_params(axis='x', labelsize=13.5)\n# plt.tick_params(axis='y', labelsize=12.5)\n\n# plt.title('Scores of Models', size=20)\n\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-21T17:57:01.184030Z","iopub.execute_input":"2022-07-21T17:57:01.184546Z","iopub.status.idle":"2022-07-21T17:57:01.422703Z","shell.execute_reply.started":"2022-07-21T17:57:01.184501Z","shell.execute_reply":"2022-07-21T17:57:01.421428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Blend models in order to make the final predictions more robust to overfitting\n\n# Submission V1: blended model without Stacking Regressor\n# def blended_predictions(X):\n#     return ((0.2 * ridge_model_full_data.predict(X)) + \\\n#             (0.2 * svr_model_full_data.predict(X)) + \\\n#             (0.2 * gbr_model_full_data.predict(X)) + \\\n#             (0.1 * xgb_model_full_data.predict(X)) + \\\n#             (0.2 * lgb_model_full_data.predict(X)) + \\\n#             (0.1 * rf_model_full_data.predict(X)))\n\n# Submission V2: blended model with Stacking Regressor\n# def blended_predictions(X):\n#     return ((0.1 * ridge_model_full_data.predict(X)) + \\\n#             (0.2 * svr_model_full_data.predict(X)) + \\\n#             (0.1 * gbr_model_full_data.predict(X)) + \\\n#             (0.1 * xgb_model_full_data.predict(X)) + \\\n#             (0.1 * lgb_model_full_data.predict(X)) + \\\n#             (0.05 * rf_model_full_data.predict(X)) + \\\n#             (0.35 * stack_gen_model.predict(np.array(X))))\n\n# Submission V3: blended model with Stacking Regressor\ndef blended_predictions(X):\n    return ((0.1 * ridge_model_full_data.predict(X)) + \\\n            (0.1 * svr_model_full_data.predict(X)) + \\\n            (0.1 * gbr_model_full_data.predict(X)) + \\\n            (0.05 * xgb_model_full_data.predict(X)) + \\\n            (0.1 * lgb_model_full_data.predict(X)) + \\\n            (0.05 * rf_model_full_data.predict(X)) + \\\n            (0.50 * stack_gen_model.predict(np.array(X))))\n\nblended_score = rmsle(y, blended_predictions(X))\nscores['blended'] = (blended_score, 0)\nprint('RMSLE score on train data:')\nprint(blended_score)","metadata":{"execution":{"iopub.status.busy":"2022-07-21T18:34:01.551348Z","iopub.execute_input":"2022-07-21T18:34:01.551730Z","iopub.status.idle":"2022-07-21T18:34:03.015165Z","shell.execute_reply.started":"2022-07-21T18:34:01.551701Z","shell.execute_reply":"2022-07-21T18:34:03.013880Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read in sample_submission dataframe\nsubmission = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/sample_submission.csv\")\nsubmission.iloc[:,1] = np.floor(np.expm1(blended_predictions(X_submission)))\n\n# Fix outleir predictions\nq1 = submission['SalePrice'].quantile(0.0045)\nq2 = submission['SalePrice'].quantile(0.99)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x > q1 else x*0.77)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x < q2 else x*1.1)\n\nsubmission.to_csv(\"submission_v3.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-07-21T18:26:58.570139Z","iopub.execute_input":"2022-07-21T18:26:58.570536Z","iopub.status.idle":"2022-07-21T18:27:00.061772Z","shell.execute_reply.started":"2022-07-21T18:26:58.570501Z","shell.execute_reply":"2022-07-21T18:27:00.060903Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
